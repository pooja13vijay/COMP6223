{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5685c7",
   "metadata": {},
   "source": [
    "# **1. Loading the Training dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e57a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552aeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"training\"\n",
    "width = 256\n",
    "height = 256\n",
    "dimension = (width, height)\n",
    "encoding = 0\n",
    "label = []\n",
    "data = []\n",
    "encoded_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1926a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, directory, files in os.walk(path):\n",
    "    \n",
    "    # example of the root drive/MyDrive/training/bedroom\n",
    "    if root != path:\n",
    "        counter = 0\n",
    "        for fp in files:\n",
    "            if fp != \".DS_Store\":\n",
    "                counter += 1\n",
    "                \n",
    "                # example of the filepath drive/MyDrive/training/bedroom/0.jpg\n",
    "                filepath = os.path.join(root, fp)\n",
    "\n",
    "                # Images are of different shapes\n",
    "                img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                # All images have the dimension (256, 256) => (width, height)\n",
    "                img = cv2.resize(img_array, dimension)\n",
    "                data.append(img)\n",
    "                encoded_labels.append(encoding)\n",
    "        \n",
    "        encoding += 1  \n",
    "        # Getting the label\n",
    "        label.append(root.split(\"/\")[-1])\n",
    "\n",
    "label = list(set(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ae2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 256, 256)\n",
      "(1500,)\n"
     ]
    }
   ],
   "source": [
    "# X contains all 1500 images\n",
    "# Shape = 1500, 256, 256\n",
    "X = np.array(data)\n",
    "print(X.shape)\n",
    "\n",
    "# y contains the labels of X\n",
    "# Shape = 1500, \n",
    "y = np.array(encoded_labels)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe538bf7",
   "metadata": {},
   "source": [
    "# 2. Spliting the Data as Training set and Validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e02c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% for the Training set and 10% for the Validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "# Shape of X_train = (1350, 256, 256)\n",
    "# Shape of y_train = (1350,)\n",
    "# Shape of X_val = (150, 256, 256)\n",
    "# Shape of y_val = (150,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc13e7",
   "metadata": {},
   "source": [
    "# 3. Using only the Training set to create the Vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dde64",
   "metadata": {},
   "source": [
    "## **3.1. Visual Feature extraction using Dense SIFT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ddcf482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift_features(X):\n",
    "    \n",
    "    # Dense SIFT with different number of key points.\n",
    "    # The whole idea of dense SIFT is to have key points over the entire image, having more irrelevant information as well\n",
    "    # Since the image size is choosen as 256x256, we can choose say step_size = 4 or 8 \n",
    "    step_size = 8\n",
    "    key_points = []\n",
    "    dense_descriptors = []\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        counter += 1\n",
    "        temp = []\n",
    "        for m in range(0, len(X[0]), step_size):\n",
    "            for n in range(0, len(X[1]), step_size):\n",
    "                temp.append(cv2.KeyPoint(m, n, step_size))\n",
    "\n",
    "        # List of key-points for each image\n",
    "        key_points_image = tuple(temp)\n",
    "        \n",
    "        # Each image has 1024 key points\n",
    "        key_points.append(key_points_image)\n",
    "\n",
    "        _, dense_features =  sift.compute(X[i], temp)\n",
    "        \n",
    "        # There are 1024 key points in each image\n",
    "        # That means there will be 128 feature descriptor for each of those 1024 key points\n",
    "        # Thus the shape will be (1024, 128) for each of the dense features\n",
    "        # There are a total of 1500 images, which means the dense_descriptor \n",
    "        # will have a shape of (1500, 1024, 128)\n",
    "        dense_descriptors.append(dense_features)\n",
    "\n",
    "    return key_points, dense_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6803e823",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'xfeatures2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0ef0af9444e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkey_points_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_descriptors_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-845705bdfee6>\u001b[0m in \u001b[0;36msift_features\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mkey_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdense_descriptors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0msift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIFT_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'xfeatures2d'"
     ]
    }
   ],
   "source": [
    "key_points_train, dense_descriptors_train = sift_features(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e6c39a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'key_points_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f2c520326a48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Key points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_points_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_points_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_points_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_points_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'key_points_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Key points\n",
    "print(len(key_points_train))\n",
    "print(type(key_points_train))\n",
    "print(len(key_points_train[0]))\n",
    "print(type(key_points_train[0]))\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Descriptors\n",
    "print(len(dense_descriptors_train))\n",
    "print(type(dense_descriptors_train))\n",
    "print(dense_descriptors_train[0].shape)\n",
    "print(type(dense_descriptors_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66876bd8",
   "metadata": {},
   "source": [
    "## **3.2. Creating the Bag of Visual Words (BoVW) or the Vocabulary:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab2767",
   "metadata": {},
   "source": [
    "### 3.2.1. Creating all the Descriptors for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9d9f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_descriptors(dense_descriptors):\n",
    "    list_of_descriptor = []\n",
    "    \n",
    "    for image_descriptor in dense_descriptors:\n",
    "        for feature_vector in image_descriptor:\n",
    "            list_of_descriptor.append(feature_vector)\n",
    "            \n",
    "    return list_of_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e921ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_descriptor_train = all_descriptors(dense_descriptors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b75b9ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1382400\n",
      "<class 'list'>\n",
      "\n",
      "128\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_descriptor_train))\n",
    "print(type(list_of_descriptor_train))\n",
    "print()\n",
    "\n",
    "print(len(list_of_descriptor_train[0]))\n",
    "print(type(list_of_descriptor_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24cbe8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   1.,   1.,   1.,   0.,   0.,   0.,   0.,   2.,  10.,\n",
      "         6.,   2.,   0.,   0.,   0.,   2.,   2.,   5.,   3.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         1.,  21.,   5.,   1.,   0.,   0.,   0.,   0.,   8., 147.,  38.,\n",
      "         5.,   0.,   0.,   1.,   4.,  15., 108.,  23.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   4.,  32.,   1.,   0.,  10.,\n",
      "       167.,   7.,   5.,  35., 106.,  30.,   7., 151., 226.,  56.,   3.,\n",
      "        37.,  46.,  12.,  17., 226., 226.,  24.], dtype=float32), array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   5.,   3.,   1.,   0.,   0.,   0.,   0.,   4.,  45.,\n",
      "        19.,   4.,   0.,   0.,   1.,   4.,   6.,  37.,  11.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,  12.,   0.,   0.,\n",
      "         2.,  79.,   3.,   2.,  13.,  39.,  12.,   3.,  57., 177.,  31.,\n",
      "         1.,  12.,  14.,   3.,   5., 127., 177.,  17.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   1.,   8.,  40.,   3.,   1.,  26.,\n",
      "        88.,   5.,   7.,  49., 142.,  40.,  14., 162., 177.,  45.,  14.,\n",
      "        50.,  64.,  27.,  35., 177., 177.,  22.], dtype=float32)]\n",
      "(1382400, 128)\n"
     ]
    }
   ],
   "source": [
    "# Since these are separate np.array's, we can convert them into a single array using numpy\n",
    "print(list_of_descriptor_train[0:2])\n",
    "\n",
    "# The shape of the output is (1350*1024, 128)\n",
    "# This is becasue when we are training on the k-means clustering algorithm, we only care about the feature vectors\n",
    "# The information regarding which image they come from is not needed\n",
    "list_of_descriptor_train = np.stack(list_of_descriptor_train)\n",
    "print(list_of_descriptor_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad369532",
   "metadata": {},
   "source": [
    "### 3.2.2. Using k-means to find the Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7018f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering is now performed and similar BoVW are grouped together \n",
    "# Then the centre of those clusters are returned \n",
    "\n",
    "k_means = KMeans(n_clusters = 200, random_state = 0, n_init = 1, verbose = 0)\n",
    "k_means.fit(list_of_descriptor_train)\n",
    "vocabulary = k_means.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc41d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<class 'numpy.ndarray'>\n",
      "(200, 128)\n"
     ]
    }
   ],
   "source": [
    "# This is our vocabulary for the training data\n",
    "print(len(vocabulary))\n",
    "print(type(vocabulary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21df93b",
   "metadata": {},
   "source": [
    "### 3.2.3. Creating a mapping between cluster centroids (vocabulary) and the descriptors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "971df0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(dense_descriptors):\n",
    "    # Map the images to the appropriate vocabulary \n",
    "    # This means we will now be storing the images as histograms and no longer as images\n",
    "    visual_words = []\n",
    "\n",
    "    for image_descriptors in dense_descriptors:\n",
    "        map_to_vocab = []\n",
    "        for descriptor in image_descriptors:\n",
    "\n",
    "            # Eucledian Distance between these descriptors and vocabulary\n",
    "            # Shape of the descriptor is (200, 128)\n",
    "            descriptor_stack = np.tile(descriptor, (200, 1))\n",
    "\n",
    "            # Shape is (200, 128)\n",
    "            difference = descriptor_stack - vocabulary\n",
    "\n",
    "            # This will have the euclidean distance between each descriptor (128,) and the visual word (200, 128)\n",
    "            e_dist = pow(((pow(difference, 2)).sum(axis = 1)), 0.5)\n",
    "\n",
    "            # Finding the index of the minimum distance and this is the cluster index that it belongs to\n",
    "            temp = list(e_dist)\n",
    "            index_of_vocab = temp.index(min(temp))\n",
    "\n",
    "            # For the specific descriptor, this will be the cluster that it maps to\n",
    "            map_to_vocab.append(index_of_vocab)\n",
    "        map_to_vocab = np.array(map_to_vocab)\n",
    "        visual_words.append(map_to_vocab)\n",
    "    return visual_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54e59bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_words = mapping(dense_descriptors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3da9ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350\n",
      "<class 'list'>\n",
      "\n",
      "1024\n",
      "<class 'numpy.ndarray'>\n",
      "[136 101  22 ...  95  95  95]\n"
     ]
    }
   ],
   "source": [
    "# The variable visual_words represents the mapping to the vocabulary\n",
    "print(len(visual_words))\n",
    "print(type(visual_words))\n",
    "print()\n",
    "\n",
    "print(len(visual_words[0]))\n",
    "print(type(visual_words[0]))\n",
    "print(visual_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da258b",
   "metadata": {},
   "source": [
    "### 3.2.4. Counting the vocabulary in the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1270fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(visual_words):\n",
    "    # Converting all the images into histograms that represents \n",
    "    # the count of the number of times that a specific vocabulary appeared in the image\n",
    "\n",
    "    frequency_vec = []\n",
    "\n",
    "    for image_visual_words in visual_words:\n",
    "\n",
    "        # 200 represents the number of cluster centroids\n",
    "        image_frequency = np.zeros(200)\n",
    "\n",
    "        # val represents the index and this index refers to the vocabulary\n",
    "        for val in image_visual_words:\n",
    "\n",
    "            # image_frequency[val] will count the number of times that vocabulary appears within an image\n",
    "            # Shape = (200,)\n",
    "            image_frequency[val] += 1\n",
    "\n",
    "        frequency_vec.append(image_frequency)\n",
    "    \n",
    "    return frequency_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3adef940",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vec = histogram(visual_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7153890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350\n",
      "<class 'list'>\n",
      "\n",
      "200\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(len(frequency_vec))\n",
    "print(type(frequency_vec))\n",
    "print()\n",
    "print(len(frequency_vec[0]))\n",
    "print(type(frequency_vec[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a94960",
   "metadata": {},
   "source": [
    "## 3.3. Training Data to feed the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "185caf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.stack(frequency_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60502fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350, 200)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617e9e4",
   "metadata": {},
   "source": [
    "## 3.4 Normalising the counts using the tf-idf formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d863d",
   "metadata": {},
   "source": [
    "# 4. Using the functions to create feature vectors for the Validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9abf87",
   "metadata": {},
   "source": [
    "## 4.1. Extracting Dense SIFT Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d600d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_points_val, dense_descriptors_val = sift_features(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d54376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "<class 'list'>\n",
      "1024\n",
      "<class 'tuple'>\n",
      "\n",
      "\n",
      "150\n",
      "<class 'list'>\n",
      "(1024, 128)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Key points\n",
    "print(len(key_points_val))\n",
    "print(type(key_points_val))\n",
    "print(len(key_points_val[0]))\n",
    "print(type(key_points_val[0]))\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Descriptors\n",
    "print(len(dense_descriptors_val))\n",
    "print(type(dense_descriptors_val))\n",
    "print(dense_descriptors_val[0].shape)\n",
    "print(type(dense_descriptors_val[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13695587",
   "metadata": {},
   "source": [
    "## 4.2. Mapping the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "943324ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_words_val = mapping(dense_descriptors_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ed19bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "<class 'list'>\n",
      "\n",
      "1024\n",
      "<class 'numpy.ndarray'>\n",
      "[ 34  34  34 ... 129 129 129]\n"
     ]
    }
   ],
   "source": [
    "# The variable visual_words represents the mapping to the vocabulary\n",
    "print(len(visual_words_val))\n",
    "print(type(visual_words_val))\n",
    "print()\n",
    "\n",
    "print(len(visual_words_val[0]))\n",
    "print(type(visual_words_val[0]))\n",
    "print(visual_words_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378a858",
   "metadata": {},
   "source": [
    "## 4.3. Creating the Histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21b5e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data input\n",
    "frequency_vec_val = histogram(visual_words_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae96d08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "<class 'list'>\n",
      "\n",
      "200\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(len(frequency_vec_val))\n",
    "print(type(frequency_vec_val))\n",
    "print()\n",
    "print(len(frequency_vec_val[0]))\n",
    "print(type(frequency_vec_val[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8199a",
   "metadata": {},
   "source": [
    "## 4.4. Validation Data to feed the Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d92bc9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = np.stack(frequency_vec_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69c5e693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 200)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(val_data.shape)\n",
    "print(type(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54add3",
   "metadata": {},
   "source": [
    "## 4.5. Normalising the Frequency count using tf-idf for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17de4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2616c1b8",
   "metadata": {},
   "source": [
    "# 5. Classification using SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18db627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(degree=8, kernel='poly')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the SVM\n",
    "svc = SVC(kernel = \"poly\", degree = 8)\n",
    "svc.fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fae1a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc.predict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f229494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80        12\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       1.00      0.40      0.57        10\n",
      "           4       0.92      0.60      0.73        20\n",
      "           5       0.50      0.17      0.25         6\n",
      "           6       0.00      0.00      0.00         8\n",
      "           7       0.00      0.00      0.00        11\n",
      "           8       1.00      0.25      0.40         8\n",
      "           9       0.08      1.00      0.14         8\n",
      "          10       1.00      0.09      0.17        11\n",
      "          11       1.00      0.27      0.43        11\n",
      "          12       0.00      0.00      0.00        12\n",
      "          13       0.25      0.12      0.17         8\n",
      "          14       0.25      0.11      0.15         9\n",
      "\n",
      "    accuracy                           0.27       150\n",
      "   macro avg       0.47      0.25      0.25       150\n",
      "weighted avg       0.52      0.27      0.30       150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ComputerVision/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/ComputerVision/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/ComputerVision/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b373785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
