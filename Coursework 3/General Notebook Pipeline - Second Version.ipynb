{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fff7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560344b",
   "metadata": {},
   "source": [
    "## Initiate varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dab7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50 # number of representative vectors\n",
    "feature_type = 'spatial_pyramid' # [\n",
    "'dense_sift', 'pyramid_dense_sift', 'spatial_pyramid']\n",
    "sift_step_size = 5\n",
    "num_level = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99ecc2",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41c94333",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'img_path': [], 'label': []}\n",
    "\n",
    "for root, dirs, files in os.walk(\"./training/\"):\n",
    "    label = os.path.basename(root)\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            data['img_path'].append(os.path.join(root, file))\n",
    "            data['label'].append(label)\n",
    "\n",
    "df_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a72289ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./training/Forest/63.jpg</td>\n",
       "      <td>Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./training/Forest/77.jpg</td>\n",
       "      <td>Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./training/Forest/88.jpg</td>\n",
       "      <td>Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./training/Forest/89.jpg</td>\n",
       "      <td>Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./training/Forest/76.jpg</td>\n",
       "      <td>Forest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   img_path   label\n",
       "0  ./training/Forest/63.jpg  Forest\n",
       "1  ./training/Forest/77.jpg  Forest\n",
       "2  ./training/Forest/88.jpg  Forest\n",
       "3  ./training/Forest/89.jpg  Forest\n",
       "4  ./training/Forest/76.jpg  Forest"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91e951",
   "metadata": {},
   "source": [
    "## 2. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3dc29d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT Visual words\n",
    "def sift(image):\n",
    "    # construct a SIFT object \n",
    "    sift = cv2.SIFT_create()\n",
    "    # find keypoints and descriptors\n",
    "    kp, des = sift.detectAndCompute(image, None)\n",
    "    \n",
    "    return des\n",
    "\n",
    "def dense_sift(image, step_size=5):\n",
    "    # read image from img_path then convert to gray scale\n",
    "    # construct a SIFT object \n",
    "    sift = cv2.SIFT_create()\n",
    "    # create dense keypoints and compute descriptors\n",
    "    kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, image.shape[0], step_size) \n",
    "                                        for y in range(0, image.shape[1], step_size)]\n",
    "    kp, des = sift.compute(image, kp)\n",
    "    return des"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65737fb1",
   "metadata": {},
   "source": [
    "## 3. Bag of visual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e4d57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_words = []\n",
    "\n",
    "for i, row in df_data.iterrows():\n",
    "    # read image from img_path then convert to gray scale\n",
    "    image = cv2.imread(row['img_path'])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # get descriptors for each image\n",
    "    visual_words.append(dense_sift(image, sift_step_size))\n",
    "    \n",
    "df_data['visual_words'] = visual_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4fd66951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>visual_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./training/Forest/63.jpg</td>\n",
       "      <td>Forest</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./training/Forest/77.jpg</td>\n",
       "      <td>Forest</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./training/Forest/88.jpg</td>\n",
       "      <td>Forest</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./training/Forest/89.jpg</td>\n",
       "      <td>Forest</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./training/Forest/76.jpg</td>\n",
       "      <td>Forest</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   img_path   label  \\\n",
       "0  ./training/Forest/63.jpg  Forest   \n",
       "1  ./training/Forest/77.jpg  Forest   \n",
       "2  ./training/Forest/88.jpg  Forest   \n",
       "3  ./training/Forest/89.jpg  Forest   \n",
       "4  ./training/Forest/76.jpg  Forest   \n",
       "\n",
       "                                        visual_words  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689997e3",
   "metadata": {},
   "source": [
    "### Create representation vectors - codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1878f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of visual words size: (4060424, 128)\n"
     ]
    }
   ],
   "source": [
    "# preparing bag of visual words\n",
    "BoVW = df_data['visual_words'].to_list()\n",
    "BoVW = np.vstack(BoVW)\n",
    "\n",
    "print('bag of visual words size:', BoVW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ad09056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "compactness, labels, centres = cv2.kmeans(BoVW, k, None, criteria, 10, flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca992559",
   "metadata": {},
   "source": [
    "### Histograms of bags of visual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db07c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_dense_sift(image, step_size=5, num_level=2):\n",
    "    # construct a SIFT object \n",
    "    sift = cv2.SIFT_create()\n",
    "    all_des = []\n",
    "    for l in range(num_level):\n",
    "        # create dense keypoints and compute descriptors\n",
    "        kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, image.shape[0], step_size) \n",
    "                                            for y in range(0, image.shape[1], step_size)]\n",
    "        kp, des = sift.compute(image, kp)\n",
    "        all_des.append(des)\n",
    "        \n",
    "        # resample image to the next level\n",
    "        image = cv2.pyrDown(image)\n",
    "    return np.vstack(all_des)\n",
    "\n",
    "def spatial_pyramid(image, codebook, k, step_size=5, num_level=2):\n",
    "    img_h, img_w = image.shape\n",
    "    concat_his = []\n",
    "    for l in range(num_level):\n",
    "        num_grid = 2**l\n",
    "        \n",
    "        grid_size_w = img_w//num_grid\n",
    "        grid_co_w = np.arange(0, img_w, grid_size_w) #coordinate of each grid\n",
    "        \n",
    "        grid_size_h = img_h//num_grid\n",
    "        grid_co_h = np.arange(0, img_h, grid_size_h) #coordinate of each grid\n",
    "        \n",
    "        for i in range(num_grid):\n",
    "            for j in range(num_grid):\n",
    "                des = dense_sift(image[grid_co_h[j]:grid_co_h[j]+grid_size_h, \n",
    "                                       grid_co_w[i]:grid_co_w[i]+grid_size_w], step_size)\n",
    "\n",
    "                his = histogram_bovw(des, codebook, k)\n",
    "                his = his * (1/2**(num_level-l)) # weight\n",
    "                concat_his.append(his)\n",
    "\n",
    "    concat_his = np.array(concat_his).ravel()\n",
    "    # normalizing\n",
    "    concat_his = concat_his / concat_his.sum()\n",
    "    return concat_his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "460ed150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_bovw(visual_words, codebook, k):\n",
    "    his = np.zeros(k)\n",
    "    for vw in visual_words:\n",
    "        # find distance from vw to each representation vector (codebook)\n",
    "        dist = np.power(np.power(np.tile(vw, (k, 1)) - codebook, 2).sum(axis=1), 0.5)\n",
    "        # min distance\n",
    "        min_codebook = dist.argsort()[0]\n",
    "        # calculate histogram\n",
    "        his[min_codebook] += 1\n",
    "    return his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72f64c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "his_bovw = []\n",
    "\n",
    "for i, row in df_data.iterrows():\n",
    "    # read image from img_path then convert to gray scale\n",
    "    image = cv2.imread(row['img_path'])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # dense SIFT\n",
    "    if feature_type == 'dense_sift':\n",
    "        his_bovw.append(histogram_bovw(dense_sift(image, sift_step_size), \n",
    "                                       centres, k))\n",
    "    \n",
    "    # pyramid dense SIFT\n",
    "    elif feature_type == 'pyramid_dense_sift':\n",
    "        his_bovw.append(histogram_bovw(pyramid_dense_sift(image, sift_step_size, \n",
    "                                                          num_level), centres, k))\n",
    "        \n",
    "    elif feature_type == 'spatial_pyramid':\n",
    "        his_bovw.append(spatial_pyramid(image, centres, k, sift_step_size, num_level))\n",
    "    \n",
    "his_bovw = np.array(his_bovw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f330eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 250)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "his_bovw = np.array(his_bovw)\n",
    "his_bovw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb64833",
   "metadata": {},
   "source": [
    "## 4. Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87fa3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6202a79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 250) (1500,)\n"
     ]
    }
   ],
   "source": [
    "X = his_bovw\n",
    "y = np.array(df_data['label'].to_list())\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9c9875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381d15a",
   "metadata": {},
   "source": [
    "### SVC (kernel: rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9d1bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(probability=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f8d9652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.67\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Coast       0.65      0.81      0.72        16\n",
      "      Forest       0.85      0.92      0.88        24\n",
      "     Highway       0.87      0.72      0.79        18\n",
      "  Insidecity       0.90      0.45      0.60        20\n",
      "    Mountain       0.77      0.85      0.81        20\n",
      "      Office       0.67      0.64      0.65        25\n",
      " OpenCountry       0.76      0.76      0.76        17\n",
      "      Street       0.72      0.76      0.74        17\n",
      "      Suburb       0.79      1.00      0.88        19\n",
      "TallBuilding       0.71      0.91      0.80        22\n",
      "     bedroom       0.55      0.35      0.43        17\n",
      "  industrial       0.61      0.42      0.50        26\n",
      "     kitchen       0.53      0.40      0.46        20\n",
      "  livingroom       0.36      0.43      0.39        21\n",
      "       store       0.44      0.67      0.53        18\n",
      "\n",
      "    accuracy                           0.67       300\n",
      "   macro avg       0.68      0.67      0.66       300\n",
      "weighted avg       0.68      0.67      0.66       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print('accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07d000d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top k accuracy\n",
    "top_k_accuracy_score(y_test, clf.predict_proba(X_test), k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0772dd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  1,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 22,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  0, 13,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  1,  9,  1,  0,  0,  3,  0,  5,  0,  0,  0,  0,  0],\n",
       "       [ 0,  2,  0,  0, 17,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 16,  0,  0,  1,  0,  2,  1,  2,  2,  1],\n",
       "       [ 2,  0,  1,  0,  0,  0, 13,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1,  1,  0,  0, 13,  0,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 19,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  1,  0,  0,  0,  0, 20,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  6,  4,  2,  3,  1],\n",
       "       [ 0,  0,  0,  0,  0,  2,  0,  1,  2,  0,  0, 11,  0,  0, 10],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  8, 10,  1],\n",
       "       [ 0,  0,  0,  0,  0,  3,  0,  0,  1,  0,  3,  2,  1,  9,  2],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  0,  1,  1,  0,  0,  2,  1, 12]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71d919",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d7e22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b78e5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':['rbf'], 'C':np.linspace(1, 10, 50)}\n",
    "parameters = {'kernel':['rbf'], 'C':np.linspace(0.001, 10, 50), 'gamma': [0.1, 1.0, 10, 100]}\n",
    "# parameters = {'C': [1, 5, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac348802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([1.00000000e-03, 2.05061224e-01, 4.09122449e-01, 6.13183673e-01,\n",
       "       8.17244898e-01, 1.02130612e+00, 1.22536735e+00, 1.42942857e+00,\n",
       "       1.63348980e+00, 1.83755102e+00, 2.04161224e+00, 2.24567347e+00,\n",
       "       2.44973469e+00, 2.65379592e+00, 2.85785714e+00, 3.06191837e+00,\n",
       "       3.26597959e+00, 3.47004082e+00, 3.67410204e+00, 3.87816327e+0...\n",
       "       5.71471429e+00, 5.91877551e+00, 6.12283673e+00, 6.32689796e+00,\n",
       "       6.53095918e+00, 6.73502041e+00, 6.93908163e+00, 7.14314286e+00,\n",
       "       7.34720408e+00, 7.55126531e+00, 7.75532653e+00, 7.95938776e+00,\n",
       "       8.16344898e+00, 8.36751020e+00, 8.57157143e+00, 8.77563265e+00,\n",
       "       8.97969388e+00, 9.18375510e+00, 9.38781633e+00, 9.59187755e+00,\n",
       "       9.79593878e+00, 1.00000000e+01]),\n",
       "                         &#x27;gamma&#x27;: [0.1, 1.0, 10, 100], &#x27;kernel&#x27;: [&#x27;rbf&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([1.00000000e-03, 2.05061224e-01, 4.09122449e-01, 6.13183673e-01,\n",
       "       8.17244898e-01, 1.02130612e+00, 1.22536735e+00, 1.42942857e+00,\n",
       "       1.63348980e+00, 1.83755102e+00, 2.04161224e+00, 2.24567347e+00,\n",
       "       2.44973469e+00, 2.65379592e+00, 2.85785714e+00, 3.06191837e+00,\n",
       "       3.26597959e+00, 3.47004082e+00, 3.67410204e+00, 3.87816327e+0...\n",
       "       5.71471429e+00, 5.91877551e+00, 6.12283673e+00, 6.32689796e+00,\n",
       "       6.53095918e+00, 6.73502041e+00, 6.93908163e+00, 7.14314286e+00,\n",
       "       7.34720408e+00, 7.55126531e+00, 7.75532653e+00, 7.95938776e+00,\n",
       "       8.16344898e+00, 8.36751020e+00, 8.57157143e+00, 8.77563265e+00,\n",
       "       8.97969388e+00, 9.18375510e+00, 9.38781633e+00, 9.59187755e+00,\n",
       "       9.79593878e+00, 1.00000000e+01]),\n",
       "                         &#x27;gamma&#x27;: [0.1, 1.0, 10, 100], &#x27;kernel&#x27;: [&#x27;rbf&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': array([1.00000000e-03, 2.05061224e-01, 4.09122449e-01, 6.13183673e-01,\n",
       "       8.17244898e-01, 1.02130612e+00, 1.22536735e+00, 1.42942857e+00,\n",
       "       1.63348980e+00, 1.83755102e+00, 2.04161224e+00, 2.24567347e+00,\n",
       "       2.44973469e+00, 2.65379592e+00, 2.85785714e+00, 3.06191837e+00,\n",
       "       3.26597959e+00, 3.47004082e+00, 3.67410204e+00, 3.87816327e+0...\n",
       "       5.71471429e+00, 5.91877551e+00, 6.12283673e+00, 6.32689796e+00,\n",
       "       6.53095918e+00, 6.73502041e+00, 6.93908163e+00, 7.14314286e+00,\n",
       "       7.34720408e+00, 7.55126531e+00, 7.75532653e+00, 7.95938776e+00,\n",
       "       8.16344898e+00, 8.36751020e+00, 8.57157143e+00, 8.77563265e+00,\n",
       "       8.97969388e+00, 9.18375510e+00, 9.38781633e+00, 9.59187755e+00,\n",
       "       9.79593878e+00, 1.00000000e+01]),\n",
       "                         'gamma': [0.1, 1.0, 10, 100], 'kernel': ['rbf']})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(SVC(), parameters)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fd5f0c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6741666666666666"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "220369b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0213061224489797, 'gamma': 100, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d15cb98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Coast       0.65      0.81      0.72        16\n",
      "      Forest       0.85      0.92      0.88        24\n",
      "     Highway       0.88      0.78      0.82        18\n",
      "  Insidecity       0.90      0.45      0.60        20\n",
      "    Mountain       0.77      0.85      0.81        20\n",
      "      Office       0.72      0.72      0.72        25\n",
      " OpenCountry       0.81      0.76      0.79        17\n",
      "      Street       0.72      0.76      0.74        17\n",
      "      Suburb       0.83      1.00      0.90        19\n",
      "TallBuilding       0.71      0.91      0.80        22\n",
      "     bedroom       0.64      0.41      0.50        17\n",
      "  industrial       0.58      0.42      0.49        26\n",
      "     kitchen       0.62      0.40      0.48        20\n",
      "  livingroom       0.41      0.52      0.46        21\n",
      "       store       0.46      0.67      0.55        18\n",
      "\n",
      "    accuracy                           0.69       300\n",
      "   macro avg       0.70      0.69      0.68       300\n",
      "weighted avg       0.70      0.69      0.68       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "print('accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cccc52",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29d0b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67ae71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ce7aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6066666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Coast       0.45      0.56      0.50        16\n",
      "      Forest       0.86      0.79      0.83        24\n",
      "     Highway       0.91      0.56      0.69        18\n",
      "  Insidecity       0.75      0.60      0.67        20\n",
      "    Mountain       0.65      0.75      0.70        20\n",
      "      Office       0.65      0.60      0.63        25\n",
      " OpenCountry       0.50      0.35      0.41        17\n",
      "      Street       0.62      0.76      0.68        17\n",
      "      Suburb       0.94      0.84      0.89        19\n",
      "TallBuilding       0.70      0.73      0.71        22\n",
      "     bedroom       0.40      0.24      0.30        17\n",
      "  industrial       0.57      0.50      0.53        26\n",
      "     kitchen       0.53      0.50      0.51        20\n",
      "  livingroom       0.32      0.62      0.42        21\n",
      "       store       0.58      0.61      0.59        18\n",
      "\n",
      "    accuracy                           0.61       300\n",
      "   macro avg       0.63      0.60      0.60       300\n",
      "weighted avg       0.63      0.61      0.61       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print('accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "801bec3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7766666666666666"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top k accuracy\n",
    "top_k_accuracy_score(y_test, clf.predict_proba(X_test), k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d6ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
